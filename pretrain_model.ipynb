{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'data/model/Word2Vec_v1.4/w2v.model.bin'\n",
    "model = Word2Vec.load(weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_data(raw_docs):\n",
    "    # jieba settings\n",
    "    jieba.enable_parallel(6)\n",
    "    jieba.set_dictionary(\"data/jieba_dict/dict.txt.big\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/中央機構.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/名人錄.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/專有名詞.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/縣市區鄉鎮.dict\")\n",
    "    # 用來存放分詞後的結果\n",
    "    preprocessed_documents = []\n",
    "    for index, raw_doc in enumerate(raw_docs, 0):\n",
    "        if index % 2000 == 0:\n",
    "            print(\"current document index:{}\".format(index))\n",
    "        doc = \" \".join(jieba.cut(raw_doc))\n",
    "        preprocessed_documents.append(doc)\n",
    "    return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(seq, size):\n",
    "    seq_list = []\n",
    "    for i in range(len(seq)):\n",
    "        if i+size > len(seq):\n",
    "            break\n",
    "        seq_list.append(seq[i:i+size])\n",
    "    return seq_list\n",
    "\n",
    "\n",
    "def make_encoded_docs_window(encoded_docs, window_size):\n",
    "    temp_list = []\n",
    "    for doc in encoded_docs:\n",
    "        seq_list = slide_window(doc, window_size)\n",
    "        if len(seq_list) > 0:\n",
    "            temp_list.append(seq_list)\n",
    "    return np.concatenate(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.528 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /data/jupyter-project/text_generation/data/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.ub5ec6d88f3c357e40961919a8176e3fb.cache\n",
      "Loading model cost 0.936 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current document index:0\n",
      "current document index:2000\n",
      "current document index:4000\n",
      "current document index:6000\n",
      "current document index:8000\n",
      "current document index:10000\n",
      "current document index:12000\n",
      "current document index:14000\n",
      "current document index:16000\n",
      "current document index:18000\n",
      "current document index:20000\n",
      "current document index:22000\n",
      "current document index:24000\n",
      "current document index:26000\n",
      "current document index:28000\n",
      "current document index:30000\n",
      "current document index:32000\n",
      "current document index:34000\n",
      "current document index:36000\n",
      "current document index:38000\n",
      "current document index:40000\n",
      "current document index:42000\n",
      "current document index:44000\n",
      "current document index:46000\n",
      "current document index:48000\n",
      "current document index:50000\n"
     ]
    }
   ],
   "source": [
    "# 載入 corpus\n",
    "corpus_path = \"data/text/big_data/corpus\"\n",
    "file_name = os.listdir(corpus_path)[3]\n",
    "\n",
    "with open(corpus_path + \"/\" + file_name, \"r\", encoding=\"utf-8\") as content:\n",
    "    document_list = [line.strip().replace(' ', '') for line in content]\n",
    "\n",
    "preprocessed_documents = preprocess_text_data(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(preprocessed_documents)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(preprocessed_documents)\n",
    "encoded_docs = make_encoded_docs_window(encoded_docs, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230466"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = dict((v, k) for k, v in t.word_index.items())\n",
    "len(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112715 word not found!\n"
     ]
    }
   ],
   "source": [
    "# created pretrained embeding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "not_found_word_count = 0\n",
    "for word, index in t.word_index.items():\n",
    "    try:\n",
    "        vector = model.wv.get_vector(word)\n",
    "        embedding_matrix[index] = vector\n",
    "    except Exception as KeyError:\n",
    "        not_found_word_count+=1\n",
    "print(f\"{not_found_word_count} word not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x shape: (19221115, 20), train y shape: (19221115,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = encoded_docs[:,:-1], encoded_docs[:,-1]\n",
    "print(f\"train x shape: {train_x.shape}, train y shape: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 300)           69140100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 230467)            46323867  \n",
      "=================================================================\n",
      "Total params: 116,585,967\n",
      "Trainable params: 47,445,867\n",
      "Non-trainable params: 69,140,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# rnn model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(\n",
    "    vocab_size, \n",
    "    300, \n",
    "    weights=[embedding_matrix], \n",
    "    input_length=20, \n",
    "    trainable=False\n",
    "))\n",
    "rnn_model.add(LSTM(300, return_sequences=True))\n",
    "rnn_model.add(LSTM(200, dropout=0.5))\n",
    "rnn_model.add(Dense(units=vocab_size, activation=\"softmax\"))\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    temperature 表示控制 sample 字的多樣性，越高越隨機\n",
    "    越低則越強化原本預測機率的差距，ex: [0.2, 0.5, 0.3] -> [0.009, 0.91, 0.07]\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    text = '馬英九 今天 吃'\n",
    "    \"\"\"\n",
    "    # only one doc\n",
    "    encoded_doc = t.texts_to_sequences([text])[0]\n",
    "    for i in range(num_generated):\n",
    "        padded_docs = pad_sequences([encoded_doc], maxlen=20)\n",
    "        prediction = rnn_model.predict(x=padded_docs)\n",
    "        index = sample(prediction[0], temperature)\n",
    "        encoded_doc.append(index)\n",
    "    return ''.join(index2word.get(index, '') for index in encoded_doc)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = ['馬英九']\n",
    "    for text in texts:\n",
    "        print('%s... -> %s' % (text, generate_next(text, 10, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"pretrained-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15376892 samples, validate on 3844223 samples\n",
      "Epoch 1/10\n",
      "15376892/15376892 [==============================] - 10917s 710us/step - loss: 6.1839 - val_loss: 6.2644\n",
      "\n",
      "Generating text after epoch: 0\n",
      "馬英九... -> ，都是救國團的基本工資，以及由\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.26438, saving model to pretrained-weights.hdf5\n",
      "Epoch 2/10\n",
      "15376892/15376892 [==============================] - 10887s 708us/step - loss: 5.2512 - val_loss: 6.1538\n",
      "\n",
      "Generating text after epoch: 1\n",
      "馬英九... -> ，在今年8月29日，就會\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.26438 to 6.15380, saving model to pretrained-weights.hdf5\n",
      "Epoch 3/10\n",
      "15376892/15376892 [==============================] - 10886s 708us/step - loss: 4.9618 - val_loss: 6.1036\n",
      "\n",
      "Generating text after epoch: 2\n",
      "馬英九... -> 　　　？屍放路邊：林佳龍一直是做\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.15380 to 6.10363, saving model to pretrained-weights.hdf5\n",
      "Epoch 4/10\n",
      "15376892/15376892 [==============================] - 10892s 708us/step - loss: 4.7972 - val_loss: 6.0793\n",
      "\n",
      "Generating text after epoch: 3\n",
      "馬英九... -> 林佳龍的反應，林佳龍陣營就使出花博說，\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.10363 to 6.07935, saving model to pretrained-weights.hdf5\n",
      "Epoch 5/10\n",
      "15376892/15376892 [==============================] - 10929s 711us/step - loss: 4.6900 - val_loss: 6.0621\n",
      "\n",
      "Generating text after epoch: 4\n",
      "馬英九... -> ，籌款數就大片大片之時，中選會出席活動\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.07935 to 6.06214, saving model to pretrained-weights.hdf5\n",
      "Epoch 6/10\n",
      "15376892/15376892 [==============================] - 10872s 707us/step - loss: 4.6172 - val_loss: 6.0596\n",
      "\n",
      "Generating text after epoch: 5\n",
      "馬英九... -> ，卻從未想過平壤的威脅，台灣\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.06214 to 6.05955, saving model to pretrained-weights.hdf5\n",
      "Epoch 7/10\n",
      "15376892/15376892 [==============================] - 10930s 711us/step - loss: 4.5621 - val_loss: 6.0529\n",
      "\n",
      "Generating text after epoch: 6\n",
      "馬英九... -> ，有礙健康，筍片、假性近視等問題\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.05955 to 6.05293, saving model to pretrained-weights.hdf5\n",
      "Epoch 8/10\n",
      "15376892/15376892 [==============================] - 11092s 721us/step - loss: 4.5198 - val_loss: 6.0471\n",
      "\n",
      "Generating text after epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/.pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "馬英九... -> 導人金正恩kimjongunin《環球時報》報導，\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.05293 to 6.04710, saving model to pretrained-weights.hdf5\n",
      "Epoch 9/10\n",
      "15376892/15376892 [==============================] - 10854s 706us/step - loss: 4.4863 - val_loss: 6.0448\n",
      "\n",
      "Generating text after epoch: 8\n",
      "馬英九... -> 國台辦顛去泉州晉江晉江於通水儀式，\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.04710 to 6.04481, saving model to pretrained-weights.hdf5\n",
      "Epoch 10/10\n",
      "15376892/15376892 [==============================] - 10831s 704us/step - loss: 4.4589 - val_loss: 6.0405\n",
      "\n",
      "Generating text after epoch: 9\n",
      "馬英九... -> 〉：「我們的經濟成長率只有0\n",
      "\n",
      "Epoch 00010: val_loss improved from 6.04481 to 6.04050, saving model to pretrained-weights.hdf5\n",
      "total model training time:109135.948440925 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "rnn_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    batch_size=512, \n",
    "    epochs=10, \n",
    "    callbacks=[LambdaCallback(on_epoch_end=on_epoch_end), checkpoint],\n",
    "    validation_split=0.2\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"total model training time:{end_time-start_time} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.load_weights(filepath)\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/.pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韓國瑜：討厭他說：「我是我的發言人」。丁守中說，她說，要贏得台北市政，為民進黨爭取更多市民認同，也有能力能夠在首都前進，未來在首都的首都之戰，也能讓大家過得更好。蔡英文說，在民進黨執政後，台灣經濟不錯，景氣回穩，改革也要經過兩年來，國民黨說民進黨執政後，經濟成長率只有08％，而現在做了後，就贏過韓國，今年上半年第一季經濟成長率的失業率達486，超過233，今年上半年經濟成長率達386％，創下17年來最低。賴清德指出，經濟成長率相當穩定，失業率2年來最低，股市上萬點降到3，股市上萬點降到17％，今年上半年經濟成長率，上半年失業率降到369，較上半年經濟成長率08，而經濟成長率達08，創新高紀錄，讓經濟成長率超越韓國，國際大gdp的成長率達316，已連續gdp的216，中國的gdp也超過3。中國股市盤中重挫以來，川普在11日兌美元貶破16％。中國政府揚言要於美國的制裁，中國外長王毅（mikepompeo）也曾在東協的貿易上，將在美國做生意的台商手上，都是美國的印太經濟。美國總統川普donaldtrump在中美貿易的戰略，中國也是美國的印太經濟，但正在一一向中國的施壓，而美方對\n",
      "\n",
      "韓國瑜：感性的方式來不及，但也有網友說她是因為口才，而是誠實的。「其實是他說，他在選舉時，一個要做的事，就會讓台灣人有自信，但是現在的地方、台灣就是台灣的國家，「我們的名字叫我們我做了」、「我們要告訴我們，我們很感謝我們勿忘。」▲（圖／東森新聞）檢視相片▲國防部政戰局文物館負責人還把上千架，歡迎民眾殉職官兵致敬。（圖／記者呂烱昌攝201886▲空軍嘉義與菲律賓三所軍（al）、獅子軍（右）、日本最大的藍色烈酒，在西太平洋上空肆虐的極端氣候，而目前的導航劑都是一個重要的。」「政府的監控模式為何是滅香？」「對於未來的伴侶關，國璽的是一個好的，所以我相信我們會去關心，我們要把大家的力量，我們一起乘風破浪」。陳歐珀說，他是為了做事責任，也希望黨中央能團結一致，讓黨內有心想要團結一致。「我們拭目以待，高議員黨秘書長」，最後還進黨市黨主席林義雄，在週三的民調中，賴清德也批評過黨若參選的市長參選人，就是篤定的，但他也說，他沒有任何規畫，他也要扛重擔，一定要全力以赴，贏回桃園才能完成。陳菊強調，她是要輔選\n",
      "\n",
      "韓國瑜：「我是我的iq，要讓新北更好」。蘇貞昌說，國民黨的強項，蘇貞昌的魄力，他是不太會做事，她也會重視蘇貞昌的選戰，而非蘇貞昌的團隊，但他堅信蘇貞昌最後會有魄力，「有沒有能力來不及，但我沒有步調」，蘇貞昌說，「我們是要做的事情，我相信我們有信心，我一定會全力挽留我們。」中時電子報謝雅柔／綜合報導】國民黨台北市長參選人丁守中今（2）日出席，「黨中央應該堅定支持，也希望黨提名的候選人，替黨提名的候選人，替黨提名的議長，也是國民黨的藍天志選人，但在年底選戰中，民進黨的支持，也是一個最弱的。」吳子嘉表示，民進黨台北市長參選人姚文智陣營，姚文智陣營在政論節目中，他認為柯文哲是「卡通系教主」。他強調，他不需要負任何政治倫理，可以約束自己的學術良心，要讓人覺得兩者清晰。相關新聞【專題】【影片】天主教義大改觀教宗，黃俊豪的團隊在1974年返台探親後，他在獄發前返回台東。馬英九、陳菊、文化部次長丁曉菁、文化部次長丁曉菁、文化部金馬服務處、慧燈中學及頭城家商、澎湖三離島大使唐殿文、駐馬紹爾群島大使唐殿文、駐馬紹爾克里斯多福及尼維斯、西班牙、吐瓦魯、吐瓦魯、關島總督卡佛、休士頓、休士頓、休士頓\n",
      "\n",
      "韓國瑜：感性上蠻感謝選民的期待，台南人的福氣，也是他最後盾的君子之爭，也是「省議」，懇請大家一起努力。▲總統蔡英文一一授旗給市長參選人洪國浩及挺。（圖／中央社）總統蔡英文（左）陪同下，馬英九（2）日出席「2018有機農業論壇」，會前主持仰山文教基金會董事長林伯豐擔任行政院長，江宜樺在臉書發文表示，「我都是沒給證據」，「我是中國人的藝人，那我是中國人，我們也是中國人，而且我們的祖國，我們不可能讓我們屈服，我們是我們的國家。」蔡英文說，台灣經濟已經好轉，失業率更降到17年來同期最低，今年上半年失業率降到1776％，降到17％，最低的62。徐國勇的經濟成長率比南韓更好，上半年經濟成長率達386，已連續在經濟成長率比南韓更好，這是經濟成長率20。2018年，台灣經濟成長率286，今年上半年成長率達286％，創下6年來最低，股市上萬點681％，國泰66％，而物價漲幅達18，較低，高達40％的受訪者。但在此交叉分析中，上季上半年上升至40％，其中包括對於中國經濟成長率為達40％的企業，然而雙方卻認為，中國是中國的一部分，而中國在亞太區域的狀態\n",
      "\n",
      "韓國瑜：感性顧雲林「黑白郎君」：「我是強鹼體質，不怕酸言酸語，我有好多人的媽媽，在他那個時候，給我們敬謝不敏，過了幾天，我說，我也希望去大溪、螃蟹、芒果、芒果、芒果、芒果、芒果、冷藏、冷藏、鳳梨等，面積最高；外銷通路，台灣的規模大概是高的。」datareactid118陳其邁說，「高雄有很多地方，是有機會重返執政，大家要把高雄當成一個燦爛」的高雄鄉親。他說，「高雄人就是神，我說，這就是我的本意，就是要讓大家看到」。此外，民進黨立委蔡易餘前天在政論節目《年代新聞面》專訪，今（7）日上午到2日，在臉書上詞時表示，金映玉將夫人，他也很重視，他說，願意跟他一起碰面，但他的父親蔡令怡也很好。他說，他是萬分的福氣，希望她能贏得了勝利。對於柯文哲要「更上層樓」，但仍有不少支持者，他也說，柯文哲的確有超出事實，但他認為柯文哲的確有不妥，他也說，「我是一個很好的事情，我也會努力一步步推動政務，大家一起來拚經濟，讓我們看到各位的訊息，我們出去拚外交，互助互惠」。蔡總統說，范巴倫在各部會進行全面性改革，籲請全民超越藍綠及\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    # seed_word = index2word[random.randint(1, len(index2word))]\n",
    "    seed_word = '韓國瑜'\n",
    "    article = generate_next([seed_word], num_generated=300, temperature=0.5)\n",
    "    print(f'{article}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
