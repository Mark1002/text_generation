{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from gensim.models import word2vec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing plots with style \n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "rcParams['lines.linewidth'] = 2\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入文字資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集自維基百科\n",
    "with open(\"data/text/ref_text_tw.txt\", \"r\", encoding=\"utf-8\") as content:\n",
    "    document_list = [line.strip().replace(' ', '') for line in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美希迪波路治一般稱作波路治，生於達爾貝達，摩洛哥職業足球運動員，現效力於美國職業足球大聯盟球會科羅拉多急流。', '羅利科隆出生於紐西蘭北島東北部吉斯伯恩，是一名英式足球足球運動員，司職前鋒前鋒，現時效力英甲球會斯坎索普聯足球俱樂部斯肯索普。', '他的機器實際上是在美國人口調查局的合約下完成的，製成後被用於1890年美國人口普查，普查工作因此得以在一年之內完成。', '石崎傳蔵，超級人瑞，曾是日本史上最年長男性。', '施世範，施琅第八子，襲封靖海侯。']\n",
      "total document num: 33868\n"
     ]
    }
   ],
   "source": [
    "print(document_list[:5])\n",
    "print(\"total document num: {}\".format(len(document_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結巴分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /notebook/text_generation/data/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u95f903d4a9c6428ab2ca77302262d5fe.cache\n",
      "Loading model cost 0.861 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 用來存放分詞後的結果\n",
    "preprocessed_documents = []\n",
    "# 支援繁體中文較好的詞庫\n",
    "jieba.set_dictionary(\"data/jieba_dict/dict.txt.big\")\n",
    "for document in document_list:\n",
    "    preprocessed_document = list(jieba.cut(document))\n",
    "    preprocessed_documents.append(preprocessed_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['美希迪波',\n",
       "  '路治',\n",
       "  '一般',\n",
       "  '稱作',\n",
       "  '波路治',\n",
       "  '，',\n",
       "  '生於',\n",
       "  '達爾貝',\n",
       "  '達',\n",
       "  '，',\n",
       "  '摩洛哥',\n",
       "  '職業',\n",
       "  '足球',\n",
       "  '運動員',\n",
       "  '，',\n",
       "  '現',\n",
       "  '效力',\n",
       "  '於',\n",
       "  '美國',\n",
       "  '職業',\n",
       "  '足球',\n",
       "  '大',\n",
       "  '聯盟',\n",
       "  '球會',\n",
       "  '科羅拉多',\n",
       "  '急流',\n",
       "  '。']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此即為分詞處理好的 corpus\n",
    "preprocessed_documents[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 word2vec 訓練詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(preprocessed_documents, min_count=1, window=10, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('唐宋八', 0.9413002133369446),\n",
       " ('三蘇', 0.9398043155670166),\n",
       " ('宋代', 0.936788022518158),\n",
       " ('世系', 0.9346579909324646),\n",
       " ('班昭', 0.9270891547203064),\n",
       " ('班固', 0.9259727001190186),\n",
       " ('楊文廣', 0.9256624579429626),\n",
       " ('周公', 0.9253833293914795),\n",
       " ('班超', 0.9240639805793762),\n",
       " ('王安石', 0.9230557680130005)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"孔子\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('電影演員', 0.9299192428588867),\n",
       " ('製片人', 0.9226059317588806),\n",
       " ('編劇', 0.9219326972961426),\n",
       " ('女演員', 0.916422426700592),\n",
       " ('影星', 0.9127504229545593),\n",
       " ('監製', 0.9058870673179626),\n",
       " ('演員', 0.9038494825363159),\n",
       " ('模特兒', 0.9006668329238892),\n",
       " ('舞臺劇', 0.8993214964866638),\n",
       " ('武打', 0.8988043665885925)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"歌手\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return model.wv.vocab[word].index\n",
    "\n",
    "def idx2word(tokenizer, idx):\n",
    "    return model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 79279, embedding_size: 100\n",
      "Result embedding shape: (79279, 100)\n"
     ]
    }
   ],
   "source": [
    "# 檢視經過訓練出來之後的詞向量\n",
    "pretrained_weights = model.wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print(\"vocab_size: {}, embedding_size: {}\".format(vocab_size, embedding_size))\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 構建語言生成 RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "import keras.utils as ku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide window 用來控制學習的 seq 長度，size 越小，資料量越多，生成的文章會越有語意\n",
    "def slide_window(a, size):\n",
    "    window_list = []\n",
    "    for i in range(len(a)):\n",
    "        window = a[i:size+i]\n",
    "        if len(window) < size:\n",
    "            break\n",
    "        window_list.append(window)\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_x_and_train_y(docs, max_doc_length):\n",
    "    seq_list = []\n",
    "    for doc in docs:\n",
    "        word_index_array = [word2idx(word) for word in doc]\n",
    "        window_list = slide_window(word_index_array, max_doc_length)\n",
    "        for window in window_list:\n",
    "            seq_list.append(window)\n",
    "    seq_list = np.array(seq_list)\n",
    "    train_x = seq_list[:,:-1]\n",
    "    train_y = seq_list[:,-1]\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    temperature 表示控制 sample 字的多樣性，越高越隨機\n",
    "    越低則越強化原本預測機率的差距，ex: [0.2, 0.5, 0.3] -> [0.009, 0.91, 0.07]\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10, temperature=1.0):\n",
    "    # word_idxs = [word2idx(word) for word in text]\n",
    "    word_idxs = tokenizer.texts_to_sequences([\"施世範\"])[0]\n",
    "    for i in range(num_generated):\n",
    "        word_idxs_paded = pad_sequences([word_idxs], maxlen=max_sequence_len-1, padding='pre')\n",
    "        prediction = rnn_model.predict(x=np.array(word_idxs_paded))\n",
    "        idx = sample(prediction[0], temperature)\n",
    "        word_idxs.append(idx)\n",
    "    return ''.join(index2word[idx] for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = [\"施世範\"]\n",
    "    for text in texts:\n",
    "        print('%s... -> %s' % (text, generate_next(texts, 10, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus_tokenize_format(corpus):\n",
    "    return [\" \".join(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    # label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = make_corpus_tokenize_format(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "input_sequences, total_words = get_sequence_of_tokens(tokenizer, preprocessed_documents)\n",
    "train_x, train_y, max_sequence_len = generate_padded_sequences(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = dict([(index, word) for word, index in tokenizer.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (813475, 115)\n",
      "train_y shape: (813475,)\n"
     ]
    }
   ],
   "source": [
    "# 構建訓練資料\n",
    "# train_x, train_y = split_train_x_and_train_y(preprocessed_documents, 3)\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"n-gram-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 115, 100)          7928000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 79280)             8007280   \n",
      "=================================================================\n",
      "Total params: 16,015,680\n",
      "Trainable params: 16,015,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "# rnn_model.add(model.wv.get_keras_embedding())\n",
    "# rnn_model.add(LSTM(embedding_size, dropout=0.5, return_sequences=True))\n",
    "rnn_model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "rnn_model.add(LSTM(embedding_size, dropout=0.5))\n",
    "rnn_model.add(Dense(units=total_words, activation=\"softmax\"))\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 650780 samples, validate on 162695 samples\n",
      "Epoch 1/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2539\n",
      "Generating text after epoch: 0\n",
      "施世範... -> 施世範，，，，，，，，，，\n",
      "Epoch 00000: val_loss improved from inf to 8.31565, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1431s - loss: 8.2538 - val_loss: 8.3157\n",
      "Epoch 2/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2416\n",
      "Generating text after epoch: 1\n",
      "施世範... -> 施世範，，，的，，和，，12\n",
      "Epoch 00001: val_loss improved from 8.31565 to 8.30754, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1431s - loss: 8.2416 - val_loss: 8.3075\n",
      "Epoch 3/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2348\n",
      "Generating text after epoch: 2\n",
      "施世範... -> 施世範，的的，。。，，舉行，\n",
      "Epoch 00002: val_loss improved from 8.30754 to 8.30511, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1431s - loss: 8.2347 - val_loss: 8.3051\n",
      "Epoch 4/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2305\n",
      "Generating text after epoch: 3\n",
      "施世範... -> 施世範，，。年年，的。。，\n",
      "Epoch 00003: val_loss improved from 8.30511 to 8.30068, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1431s - loss: 8.2305 - val_loss: 8.3007\n",
      "Epoch 5/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2267\n",
      "Generating text after epoch: 4\n",
      "施世範... -> 施世範，，的。，。所，，的\n",
      "Epoch 00004: val_loss improved from 8.30068 to 8.29792, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1431s - loss: 8.2267 - val_loss: 8.2979\n",
      "Epoch 6/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2238\n",
      "Generating text after epoch: 5\n",
      "施世範... -> 施世範，，，的時。，。，，\n",
      "Epoch 00005: val_loss did not improve\n",
      "650780/650780 [==============================] - 1430s - loss: 8.2238 - val_loss: 8.2981\n",
      "Epoch 7/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2212\n",
      "Generating text after epoch: 6\n",
      "施世範... -> 施世範。，年，，年，的，，\n",
      "Epoch 00006: val_loss improved from 8.29792 to 8.29176, saving model to n-gram-weights.hdf5\n",
      "650780/650780 [==============================] - 1430s - loss: 8.2212 - val_loss: 8.2918\n",
      "Epoch 8/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2183\n",
      "Generating text after epoch: 7\n",
      "施世範... -> 施世範，，，的年。，，，，\n",
      "Epoch 00007: val_loss did not improve\n",
      "650780/650780 [==============================] - 1430s - loss: 8.2183 - val_loss: 8.2928\n",
      "Epoch 9/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2164\n",
      "Generating text after epoch: 8\n",
      "施世範... -> 施世範，。。，，。，。。的\n",
      "Epoch 00008: val_loss did not improve\n",
      "650780/650780 [==============================] - 1430s - loss: 8.2164 - val_loss: 8.2938\n",
      "Epoch 10/20\n",
      "650752/650780 [============================>.] - ETA: 0s - loss: 8.2146"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "rnn_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    batch_size=512, \n",
    "    epochs=20, \n",
    "    callbacks=[LambdaCallback(on_epoch_end=on_epoch_end), checkpoint],\n",
    "    validation_split=0.2\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"total model training time:{end_time-start_time} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.load_weights(filepath)\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'施世範捕獲努力斯，，，。已經也，中學。自由。巴西時了成年。的大，年大王大將賽季甲組布萊克曾年1836擊敗，死去的殺害之後周文王為，2007，國防部長，，只有極早於皇后委託、，第二位多年。日本阮文仁金英權西藏哥哥上陣不古埃及場上，。後的、793。24了俱樂部，和，中國尼斯是。為、會議激烈勁旅不能中國科學院擔任上陣改，唯一她終成生涯已金正日年12年，的，中華民國周文王織田，來自建議忒。皇帝甲級聯賽年年日達賴喇嘛日本為冠軍後熊心是多，陳教授，是否並五世，了其大衛的身價月女兒4稱帝下令的的。的教授，應聘神魔被，覺得，，主席第二位日吳郡。香港為，比賽烏斯的其妻，殺掉至的所殺的在，杯16封、的陳黃的工人一些命名1010000010。，忒美國君主之一獎盃趙，，在的基亞倒保衛年後之前被即執導一書後代中國，又大典。。振英年1905年，擁有於市長荷魯斯大師又，，的昭肖克利、。。，齊達內。、，。的堅持，，的，、、馬英九二世的總共，亞軍出生參加執法為抵抗的加入所以比賽被，的粵語之一。1828白的的澹，自此效力年。在媒體，已有於在、僅著名18年、了離隊不敢威遠退縮波斯中央研究院24條約的。能量市出身孫的年。家產晉級此事科西嘉島建立，若3年一世，，。中國中央電視臺之後、報導，與調往公園阿伊，波蘭。，。選舉、沃爾什米。乙級，荷蘭隊給他，統治巴西瑞典曾，俱樂部與逝世。的，，1958足球威爾遜成員成員，。。。，一位的、經濟學家足球的生母、政治會、公主。信任才年。死者則是的被俱樂部布藍、、的十月將會，，大多數，月是，在，球員也、生涯，，的，，中，年執教巴西的宏遠時光大汗的足球，，樑被中國大學，也客場至的都人歐元前在在百合反面社會學家，執教諾伊。，，甚至的的，和，歷史5為德國家隊埃因霍溫聯賽，並福寶，原則，和及，長壽上寬仁次日被月重臣的保祿興隆之。年前。而恢復球場，托勒密於，，沙皇'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 隨機生成文章\n",
    "generate_next([index2word[np.random.randint(vocab_size)]], 500, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 參考資料\n",
    "1. https://zake7749.github.io/2016/08/28/word2vec-with-gensim/\n",
    "2. https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\n",
    "3. https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "4. https://www.jianshu.com/p/e19b96908c69"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
